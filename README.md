# Подготовка к собеседованию для Spark Data Engineer
## Spark
### Структура Spark
### Spark Core
### Spark SQL 
### Spark Streaming
### Spark Catalist
### Отличия Spark и PySpark
* Основным отличием является обработка UDF, в том плане, что Python работает кратно медленнее, чем Scala
* Некоторые методы работают иначе, у некоторых разные параметры, но это надо точечно сравнить, общей инфы нет
### Ect
### Map-Reduce как аналог Spark
## Scala
### UDF
## Python
## Sql
### Joinы (включая spark)
### Оконки
### Greenplam
## Оркестраторы 
* Задачи решаемые оркестратором
  * Планирование задач — основная функция, позволяющая избавиться от ручного запуска рутинных задач по расчёту витрин, загрузке данных, резервному копированию
  * Управление зависимостями. Часто задачу нужно запустить не только в определённый промежуток времени, но и с учётом статуса других задач
  * Репроцессинг. Если известно, что какая-то задача требует перезапуска (например, были загружены неполные данные на предыдущем этапе), то перезапуска требуют и задачи, зависящие от неё. Кроме того, перезапуск может быть необходим за несколько временных периодов
  * Мониторинг
### Airflow
* В основе концепции Airflow лежит DAG — направленный ациклический граф. Он описывает процессы обработки данных и позволяет объединять задачи, определяя правила их совместной работы
  * https://airflow.apache.org/
* Airflow состоит из нескольких компонентов, но главные из них — scheduler и webserver. Без них ничего не запустится
  * Scheduler (планировщик) отслеживает все задачи и DAGs, а затем запускает экземпляры задач после установки их зависимостей. В фоновом режиме планировщик запускает подпроцесс, который отслеживает и синхронизирует все DAGs в указанном каталоге. По умолчанию он раз в минуту собирает результаты синтаксического анализа DAGs и проверяет, необходимо ли запустить какие-либо активные задачи
    * https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/scheduler.html
  * WebServer (веб-сервер) отвечает за отображение веб-интерфейса и аутентификацию пользователей, а также решает, к какой группе должен относиться тот или иной пользователь в соответствии с конфигурационным файлом
    * https://airflow.apache.org/docs/apache-airflow/stable/security/webserver.html
### Oozie
### Luigi
### NIFI
## Прочие полезности
### Yarn
### Hadoop
### Casandra
### Kafka
### ClickHouse
## Справочная информация
### Принципы и подходы разработки
### Dwh, DataLake, DataVault
### Форматы данных в BigData
* **Линейные** (строковые)
  * Строки данных хранятся вместе образуя непрерывное хранилище
  * Пониженная скорость чтения и выполнения изберательных запросов
  * Большой расход дискового пространства
  * Лучше подходят для потоковой записи
  * Виды
    * `AVRO`
      * Может использовать компактную бинарную кодировку или человекочитаемый формат JSON  
      * Обеспечивает высокую скорость записи
      * Подходит для Apache Kafka, Flume, DataLake
    * `Sequence`
      * Двоичный формат для хранения данных в виде сериализованных пар ключ/значение 
      * Обеспечивает хороший параллелизм при MapReduce  
* **Колоночные** (столбцовые)
  * Столбцы хранятся вместе, но могут обрабатываться отдельно друг от друга
  * Быстрое чтение данных в изберательных запросах
  * Занимает много оперативной памяти
  * Не подходит для потоковой записи, т.к. файл не может быть восстановлен из-за отсутствия точек синхронизации
  * Занимает меньше дискового пространства
  * Виды
    * `Parquet`
      * Бинарный, колоночно-ориентированный формат
      * Подходит для Kafka, Spark и Hadoop
      * Поддерживает boolean, int32, int64, int96, float, double, byte_array
      * *Достоинства*
        * Экономия места
        * Высокая скорость
        * Возможность реализации собственных схем данных
        * Многие ЯП поддерживают
        * Возможность хранения данных не только в HDFS
        * Простота и удобство работы с файлами
        * Поддержка Apache Spark «по умолчанию»
      * *Недостатки*
        * Строгая типизация данных
        * Отсутствие встроенной (нативной) поддержки в других фреймворках, кроме Apache Spark
        * Отсутствие возможности отслеживать изменение данных и эволюцию схемы
        * Сложность частичной потоковой передачи данных
        * Сильная привязка к метаданным
        * Паркет не является человекочитаемым форматом
    * `RCFile`
      * Гибридный многоколонный формат записей, адаптированный для хранения реляционных таблиц на кластерах и предназначенный для систем использующих MapReduce
    * `ORC`
      * *Достоинства*
        * Один файл на выходе любой задачи — это снижает нагрузку на узел имен (NameNode)
        * Поддерживается тип данных Hive, сложные и десятичные типы данных (list, map, struct, union)
        * Возможно одновременное считывание того же файла различными процессами RecordReader
        * Можно разделить файлы без сканирования этих файлов на предмет наличия маркеров
        * Индексация блоков для каждого столбца
        * Генерация наиболее эффективного графа при оптимизации SQL-запросов
### Алгоритмы
### Алгоритмы кластеризации
* Формально это больше про ML, но не помешает знать, что такое существует
* Кластеризация (или кластерный анализ) — это задача разбиения множества объектов на группы, называемые кластерами. Внутри каждой группы должны оказаться «похожие» объекты, а объекты разных группы должны быть как можно более отличны. Главное отличие кластеризации от классификации состоит в том, что перечень групп четко не задан и определяется в процессе работы алгоритма
* Виды:
  * `Иерархический`
  * `k-средних`
  * `с-средних`
  * `Выделение связных компонент`
  * `Минимально покрывающее дерево`
  * `Послойная кластеризация`
* Ссылка на статью - https://habr.com/ru/articles/101338/
## Остальное 
### Docker
### kubernets 
# Раскаиваться что первую итеррацию забыл сохранить =)
